{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 든든히 먹고\n",
      "튼튼히 크는\n",
      "서울든든급식# SE♡UL M! SOUL# 서울특별시# < 진행 순서 >| 내 용 | 진 행 |\n",
      "| --- | --- |\n",
      "| 든든급식 개편사항 안내(10\n"
     ]
    }
   ],
   "source": [
    "with open(\"9f9e37a6-55f4-43c4-8285-b4b976f5dd4b.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(content[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap =10,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = text_splitter.create_documents([content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch에 성공적으로 연결되었습니다!\n",
      "Elasticsearch 버전: 9.0.2\n"
     ]
    }
   ],
   "source": [
    "# Elasticsearch BM25 구현\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "# Elasticsearch 클라이언트 연결 (9.x 버전 호환)\n",
    "es = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "# 연결 확인\n",
    "try:\n",
    "    if es.ping():\n",
    "        print(\"Elasticsearch에 성공적으로 연결되었습니다!\")\n",
    "        # 클러스터 정보 출력\n",
    "        info = es.info()\n",
    "        print(f\"Elasticsearch 버전: {info['version']['number']}\")\n",
    "    else:\n",
    "        print(\"Elasticsearch 연결에 실패했습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"Elasticsearch 연결 오류: {e}\")\n",
    "    print(\"Docker Compose로 Elasticsearch가 실행 중인지 확인해주세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 인덱스 'bm25_documents'를 삭제했습니다.\n",
      "인덱스 'bm25_documents'를 생성했습니다.\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 생성 (BM25 설정 포함)\n",
    "index_name = \"bm25_documents\"\n",
    "\n",
    "# 인덱스 매핑 정의 (BM25 파라미터 설정)\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"similarity\": {\n",
    "                \"bm25_similarity\": {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,    # 기본값\n",
    "                    \"b\": 0.75     # 기본값\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"bm25_similarity\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            },\n",
    "            \"chunk_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 인덱스가 존재하면 삭제\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"기존 인덱스 '{index_name}'를 삭제했습니다.\")\n",
    "\n",
    "# 인덱스 생성\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "print(f\"인덱스 '{index_name}'를 생성했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 인덱싱 완료\n",
      "Chunk 1 인덱싱 완료\n",
      "Chunk 2 인덱싱 완료\n",
      "Chunk 3 인덱싱 완료\n",
      "Chunk 4 인덱싱 완료\n",
      "Chunk 5 인덱싱 완료\n",
      "Chunk 6 인덱싱 완료\n",
      "Chunk 7 인덱싱 완료\n",
      "Chunk 8 인덱싱 완료\n",
      "Chunk 9 인덱싱 완료\n",
      "Chunk 10 인덱싱 완료\n",
      "Chunk 11 인덱싱 완료\n",
      "Chunk 12 인덱싱 완료\n",
      "Chunk 13 인덱싱 완료\n",
      "Chunk 14 인덱싱 완료\n",
      "Chunk 15 인덱싱 완료\n",
      "Chunk 16 인덱싱 완료\n",
      "Chunk 17 인덱싱 완료\n",
      "Chunk 18 인덱싱 완료\n",
      "Chunk 19 인덱싱 완료\n",
      "Chunk 20 인덱싱 완료\n",
      "Chunk 21 인덱싱 완료\n",
      "Chunk 22 인덱싱 완료\n",
      "총 23개의 chunk가 인덱싱되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 문서 chunk들을 Elasticsearch에 인덱싱\n",
    "for i, doc in enumerate(docs):\n",
    "    document = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"chunk_id\": i\n",
    "    }\n",
    "    \n",
    "    # 문서 인덱싱\n",
    "    response = es.index(index=index_name, id=i, body=document)\n",
    "    print(f\"Chunk {i} 인덱싱 완료\")\n",
    "\n",
    "# 인덱스 새로고침 (검색 가능하도록)\n",
    "es.indices.refresh(index=index_name)\n",
    "print(f\"총 {len(docs)}개의 chunk가 인덱싱되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어: '서울든든급식'\n",
      "검색 결과 (3개):\n",
      "==================================================\n",
      "[결과 1] Chunk ID: 7, BM25 점수: 2.8021\n",
      "내용: 11월 20일(수)부터 '24년 12월분 식재료 주문이 가능합니다.\n",
      "\n",
      "성동구 어린이집 대상 신규 회원가입 안내 >\n",
      "\n",
      "회원가입 후 12월 20일(금) 일괄 승인될 예정이며,\n",
      "\n",
      "승인완료 시에는 [승인완료] 문자가 발송됩니다.\n",
      "\n",
      "12월 20일(금)부터 '25년 1월분 식재료 주문이 가능합니다.\n",
      "\n",
      "매월 20일까지 신청 시 익월 1일분부터 주문 가능합니다.\n",
      "\n",
      "- 서...\n",
      "하이라이트: <em>서울든든급식</em> 신청 바로가기\n",
      "\n",
      "# 1-3....\n",
      "------------------------------\n",
      "[결과 2] Chunk ID: 0, BM25 점수: 2.6964\n",
      "내용: # 든든히 먹고\n",
      "튼튼히 크는\n",
      "서울든든급식# SE♡UL M! SOUL# 서울특별시# < 진행 순서 >| 내 용 | 진 행 |\n",
      "| --- | --- |\n",
      "| 든든급식 개편사항 안내(10분) | 서울시 친환경급식과 공공급식팀장 |\n",
      "| Q&A(10분) |  |\n",
      "| 든든급식 웹쇼핑몰 사용법 (30분) | 나라원시스템 |\n",
      "| Q&A(10분) |  |\n",
      "\n",
      "\n",
      "' \"\n",
      "서울든든...\n",
      "하이라이트: # 든든히 먹고\n",
      "튼튼히 크는\n",
      "<em>서울든든급식</em># SE♡UL M!...\n",
      "------------------------------\n",
      "[결과 3] Chunk ID: 6, BM25 점수: 2.5700\n",
      "내용: # 4. 식재료 주문 방법# 1 식재료주문 서울든든급식 웹쇼핑몰 이용# · 발주기한 납품일 기준 7일 전까지- x (은평·송파) '24.12월 2일분 주문 시 11월 20일~ 25일까지 주문 가능\n",
      "- (성동) '25. 1월 2일분 주문 시 12월 20일~26일까지 주문 가능\n",
      "- · 발주 취소 및 변경 납품일 기준 3일 전까지\n",
      "- · 클레임 요청 시에는, 배...\n",
      "하이라이트: 식재료 주문 방법# 1 식재료주문 <em>서울든든급식</em> 웹쇼핑몰 이용# · 발주기한 납품일 기준 7일 전까지- x (은평·송파) '24.12월 2일분 주문 시 11월 20일~ 25일까지 주문 가능\n",
      "- (성동) '25. 1월 2일분 주문 시 12월 20일~26일까...\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# BM25 검색 함수 정의\n",
    "def bm25_search(query, top_k=3):\n",
    "    \"\"\"\n",
    "    BM25 알고리즘을 사용하여 문서 검색\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 쿼리\n",
    "        top_k (int): 반환할 상위 결과 개수\n",
    "    \n",
    "    Returns:\n",
    "        list: 검색 결과와 점수\n",
    "    \"\"\"\n",
    "    search_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": {\n",
    "                    \"query\": query,\n",
    "                    \"operator\": \"or\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"content\", \"chunk_id\"],\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"content\": {\n",
    "                    \"fragment_size\": 150,\n",
    "                    \"number_of_fragments\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 검색 실행\n",
    "    response = es.search(index=index_name, body=search_body)\n",
    "    \n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result = {\n",
    "            'chunk_id': hit['_source']['chunk_id'],\n",
    "            'score': hit['_score'],\n",
    "            'content': hit['_source']['content'],\n",
    "            'highlight': hit.get('highlight', {}).get('content', [])\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 검색 테스트\n",
    "test_query = \"서울든든급식\"\n",
    "search_results = bm25_search(test_query, top_k=3)\n",
    "\n",
    "print(f\"검색어: '{test_query}'\")\n",
    "print(f\"검색 결과 ({len(search_results)}개):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"[결과 {i}] Chunk ID: {result['chunk_id']}, BM25 점수: {result['score']:.4f}\")\n",
    "    print(f\"내용: {result['content'][:200]}...\")\n",
    "    if result['highlight']:\n",
    "        print(f\"하이라이트: {result['highlight'][0][:150]}...\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다양한 검색어로 BM25 테스트\n",
      "============================================================\n",
      "\n",
      "🔍 검색어: '디지털 기술'\n",
      "  [1] Chunk 0 - 점수: 3.1239\n",
      "      내용: # 「AX브릿지위원회」 소개자료(벤처기업협회 회원소통본부, 2024.10.11.)\n",
      "\n",
      "데이터, 네트워크, AI 등 첨단 디지털 기술 분야 선도벤처와 유망 스\n",
      "타트업이 함께 모여 디지...\n",
      "----------------------------------------\n",
      "\n",
      "🔍 검색어: '스타트업'\n",
      "  검색 결과가 없습니다.\n",
      "----------------------------------------\n",
      "\n",
      "🔍 검색어: 'AX브릿지위원회'\n",
      "  [1] Chunk 0 - 점수: 0.7727\n",
      "      내용: # 「AX브릿지위원회」 소개자료(벤처기업협회 회원소통본부, 2024.10.11.)\n",
      "\n",
      "데이터, 네트워크, AI 등 첨단 디지털 기술 분야 선도벤처와 유망 스\n",
      "타트업이 함께 모여 디지...\n",
      "  [2] Chunk 1 - 점수: 0.6386\n",
      "      내용: # □ 운영조직- ○ 의 장 : 이주완 대표(메가존클라우드)\n",
      "- ○ 운영위원 : 10명 내외(추가섭외중)\n",
      "- ○ 사 무 국 : 벤처기업협회 회원소통본부\n",
      "# □ 주요활동- 1. (정...\n",
      "----------------------------------------\n",
      "\n",
      "🔍 검색어: '네트워크 AI'\n",
      "  [1] Chunk 0 - 점수: 1.7146\n",
      "      내용: # 「AX브릿지위원회」 소개자료(벤처기업협회 회원소통본부, 2024.10.11.)\n",
      "\n",
      "데이터, 네트워크, AI 등 첨단 디지털 기술 분야 선도벤처와 유망 스\n",
      "타트업이 함께 모여 디지...\n",
      "  [2] Chunk 1 - 점수: 1.2759\n",
      "      내용: # □ 운영조직- ○ 의 장 : 이주완 대표(메가존클라우드)\n",
      "- ○ 운영위원 : 10명 내외(추가섭외중)\n",
      "- ○ 사 무 국 : 벤처기업협회 회원소통본부\n",
      "# □ 주요활동- 1. (정...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 추가 검색 테스트 - 다양한 쿼리로 테스트\n",
    "test_queries = [\n",
    "    \"디지털 기술\",\n",
    "    \"스타트업\",\n",
    "    \"AX브릿지위원회\",\n",
    "    \"네트워크 AI\"\n",
    "]\n",
    "\n",
    "print(\"다양한 검색어로 BM25 테스트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n🔍 검색어: '{query}'\")\n",
    "    results = bm25_search(query, top_k=2)\n",
    "    \n",
    "    if results:\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "            print(f\"      내용: {result['content'][:100]}...\")\n",
    "    else:\n",
    "        print(\"  검색 결과가 없습니다.\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 쿼리: '벤처기업협회'\n",
      "==================================================\n",
      "\n",
      "📊 기본 BM25 파라미터 (k1=1.2, b=0.75):\n",
      "  [1] Chunk 1 - 점수: 0.4632\n",
      "  [2] Chunk 0 - 점수: 0.3976\n",
      "\n",
      "📊 조정된 BM25 파라미터 (k1=2.0, b=0.5):\n",
      "인덱스 'bm25_documents' 닫는 중...\n",
      "인덱스 'bm25_documents' 다시 여는 중...\n",
      "✅ BM25 파라미터 업데이트 완료: k1=2.0, b=0.5\n",
      "  [1] Chunk 1 - 점수: 0.5085\n",
      "  [2] Chunk 0 - 점수: 0.3894\n",
      "\n",
      "📊 기본값으로 복구:\n",
      "인덱스 'bm25_documents' 닫는 중...\n",
      "인덱스 'bm25_documents' 다시 여는 중...\n",
      "✅ BM25 파라미터 업데이트 완료: k1=1.2, b=0.75\n"
     ]
    }
   ],
   "source": [
    "# BM25 파라미터 조정 테스트 (수정된 버전)\n",
    "def update_bm25_parameters(k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    BM25 파라미터를 조정하는 함수 (인덱스 닫기/열기 방식)\n",
    "    \n",
    "    Args:\n",
    "        k1 (float): term frequency 조정 파라미터 (기본값: 1.2)\n",
    "        b (float): document length normalization 파라미터 (기본값: 0.75)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. 인덱스 닫기\n",
    "        print(f\"인덱스 '{index_name}' 닫는 중...\")\n",
    "        es.indices.close(index=index_name)\n",
    "        \n",
    "        # 2. 설정 업데이트\n",
    "        settings = {\n",
    "            \"index\": {\n",
    "                \"similarity\": {\n",
    "                    \"bm25_similarity\": {\n",
    "                        \"type\": \"BM25\",\n",
    "                        \"k1\": k1,\n",
    "                        \"b\": b\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        es.indices.put_settings(index=index_name, body=settings)\n",
    "        \n",
    "        # 3. 인덱스 다시 열기\n",
    "        print(f\"인덱스 '{index_name}' 다시 여는 중...\")\n",
    "        es.indices.open(index=index_name)\n",
    "        \n",
    "        # 4. 인덱스 새로고침\n",
    "        es.indices.refresh(index=index_name)\n",
    "        \n",
    "        print(f\"✅ BM25 파라미터 업데이트 완료: k1={k1}, b={b}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ BM25 파라미터 업데이트 실패: {e}\")\n",
    "        # 인덱스가 닫혀있을 수 있으니 다시 열어주기\n",
    "        try:\n",
    "            es.indices.open(index=index_name)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 서로 다른 BM25 파라미터로 테스트\n",
    "query = \"벤처기업협회\"\n",
    "print(f\"테스트 쿼리: '{query}'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 기본 파라미터 (k1=1.2, b=0.75)\n",
    "print(\"\\n📊 기본 BM25 파라미터 (k1=1.2, b=0.75):\")\n",
    "results = bm25_search(query, top_k=2)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "\n",
    "# 파라미터 조정 1 (k1=2.0, b=0.5)\n",
    "print(\"\\n📊 조정된 BM25 파라미터 (k1=2.0, b=0.5):\")\n",
    "update_bm25_parameters(k1=2.0, b=0.5)\n",
    "results = bm25_search(query, top_k=2)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "\n",
    "# 기본값으로 복구\n",
    "print(\"\\n📊 기본값으로 복구:\")\n",
    "update_bm25_parameters(k1=1.2, b=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 인덱싱된 문서 내용 확인:\n",
      "==================================================\n",
      "\n",
      "[Chunk 0]\n",
      "길이: 470 문자\n",
      "내용: # 「AX브릿지위원회」 소개자료(벤처기업협회 회원소통본부, 2024.10.11.)\n",
      "\n",
      "데이터, 네트워크, AI 등 첨단 디지털 기술 분야 선도벤처와 유망 스\n",
      "타트업이 함께 모여 디지털시대의 경쟁우위 확보를 위해 필요한 전략\n",
      "적 협력을 추구하고 규제 정비와 정책적 지원을 ...\n",
      "------------------------------\n",
      "\n",
      "[Chunk 1]\n",
      "길이: 997 문자\n",
      "내용: # □ 운영조직- ○ 의 장 : 이주완 대표(메가존클라우드)\n",
      "- ○ 운영위원 : 10명 내외(추가섭외중)\n",
      "- ○ 사 무 국 : 벤처기업협회 회원소통본부\n",
      "# □ 주요활동- 1. (정기포럼) 국내외 업계동향 및 선도벤처 기업사례 중심의 정보교류 목\n",
      "- 적의 정기포럼 2회(...\n",
      "------------------------------\n",
      "\n",
      "[Chunk 2]\n",
      "길이: 760 문자\n",
      "내용: | <운영위원> ○ 아시아 태평양 고성장 기업 3년 연속 선정 ○ 아시아' 30세 이하 리더 30인 선정 (포브스) ○ 직가맹점 200개 ↑ 해외 100호점 돌파 ○ 올해 매장 수 500호점 돌파 예정 ○ 누적 투자 유치 600억원 |\n",
      "| 4 | (주)뤼튼 테크놀로 지...\n",
      "------------------------------\n",
      "\n",
      "[Chunk 3]\n",
      "길이: 856 문자\n",
      "내용: | 순서 | 소속/성명 | 사진 | 주요약력 |\n",
      "| --- | --- | --- | --- |\n",
      "| 8 | (주)스파이어 테크놀로지 강군화 대표이사 | ![image](/image/placeholder)\n",
      " | <운영위원> ○ (주)스파이어테크놀로지, (주)스테이지7 대표...\n",
      "------------------------------\n",
      "\n",
      "📊 통계:\n",
      "- 총 문서 수: 4\n",
      "- 평균 문서 길이: 771 문자\n",
      "- 최단 문서: 470 문자\n",
      "- 최장 문서: 997 문자\n"
     ]
    }
   ],
   "source": [
    "# 인덱싱된 문서들 확인\n",
    "print(\"📄 인덱싱된 문서 내용 확인:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n[Chunk {i}]\")\n",
    "    print(f\"길이: {len(doc.page_content)} 문자\")\n",
    "    print(f\"내용: {doc.page_content[:150]}...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 통계 정보\n",
    "print(f\"\\n📊 통계:\")\n",
    "print(f\"- 총 문서 수: {len(docs)}\")\n",
    "print(f\"- 평균 문서 길이: {sum(len(doc.page_content) for doc in docs) / len(docs):.0f} 문자\")\n",
    "print(f\"- 최단 문서: {min(len(doc.page_content) for doc in docs)} 문자\")\n",
    "print(f\"- 최장 문서: {max(len(doc.page_content) for doc in docs)} 문자\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 인덱스 생성 실패: BadRequestError(400, 'illegal_argument_exception', 'No enum constant org.apache.lucene.analysis.ko.POS.Tag.E')\n",
      "Nori 플러그인이 설치되지 않았을 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 한글 최적화 BM25 인덱스 생성 (Nori 분석기 사용)\n",
    "korean_index_name = \"bm25_korean_documents\"\n",
    "\n",
    "# Nori 분석기가 포함된 인덱스 매핑 정의\n",
    "korean_mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"similarity\": {\n",
    "                \"korean_bm25_similarity\": {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,\n",
    "                    \"b\": 0.75\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"nori_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"nori_tokenizer\",\n",
    "                    \"filter\": [\n",
    "                        \"nori_part_of_speech\",\n",
    "                        \"nori_readingform\",\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"nori_tokenizer\": {\n",
    "                    \"type\": \"nori_tokenizer\",\n",
    "                    \"decompound_mode\": \"mixed\"\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"nori_part_of_speech\": {\n",
    "                    \"type\": \"nori_part_of_speech\",\n",
    "                    \"stoptags\": [\n",
    "                        \"E\", \"IC\", \"J\", \"MAG\", \"MAJ\", \"MM\", \n",
    "                        \"SP\", \"SSC\", \"SSO\", \"SC\", \"SE\", \"XPN\", \"XSA\", \"XSN\", \"XSV\",\n",
    "                        \"UNA\", \"NA\", \"VSV\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"korean_bm25_similarity\",\n",
    "                \"analyzer\": \"nori_analyzer\",\n",
    "                \"search_analyzer\": \"nori_analyzer\"\n",
    "            },\n",
    "            \"chunk_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 기존 한글 인덱스가 존재하면 삭제\n",
    "if es.indices.exists(index=korean_index_name):\n",
    "    es.indices.delete(index=korean_index_name)\n",
    "    print(f\"기존 한글 인덱스 '{korean_index_name}'를 삭제했습니다.\")\n",
    "\n",
    "# 한글 최적화 인덱스 생성\n",
    "try:\n",
    "    es.indices.create(index=korean_index_name, body=korean_mapping)\n",
    "    print(f\"✅ 한글 최적화 인덱스 '{korean_index_name}'를 생성했습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 인덱스 생성 실패: {e}\")\n",
    "    print(\"Nori 플러그인이 설치되지 않았을 수 있습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram 방식으로 한글 최적화 BM25 구현 (Nori 대안)\n",
    "ngram_index_name = \"bm25_ngram_documents\"\n",
    "\n",
    "# N-gram 기반 인덱스 매핑 정의\n",
    "ngram_mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"similarity\": {\n",
    "                \"ngram_bm25_similarity\": {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,\n",
    "                    \"b\": 0.75\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"korean_ngram_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"keyword\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"korean_ngram_filter\"\n",
    "                    ]\n",
    "                },\n",
    "                \"korean_search_analyzer\": {\n",
    "                    \"type\": \"custom\", \n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"korean_ngram_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"korean_ngram_filter\": {\n",
    "                    \"type\": \"ngram\",\n",
    "                    \"min_gram\": 2,\n",
    "                    \"max_gram\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"ngram_bm25_similarity\",\n",
    "                \"analyzer\": \"korean_ngram_analyzer\",\n",
    "                \"search_analyzer\": \"korean_search_analyzer\"\n",
    "            },\n",
    "            \"chunk_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# N-gram 인덱스 생성\n",
    "if es.indices.exists(index=ngram_index_name):\n",
    "    es.indices.delete(index=ngram_index_name)\n",
    "    print(f\"기존 N-gram 인덱스 '{ngram_index_name}'를 삭제했습니다.\")\n",
    "\n",
    "es.indices.create(index=ngram_index_name, body=ngram_mapping)\n",
    "print(f\"✅ N-gram 인덱스 '{ngram_index_name}'를 생성했습니다.\")\n",
    "\n",
    "# N-gram 인덱스에 문서 인덱싱\n",
    "print(\"N-gram 인덱스에 문서 인덱싱 중...\")\n",
    "for i, doc in enumerate(docs):\n",
    "    document = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"chunk_id\": i\n",
    "    }\n",
    "    es.index(index=ngram_index_name, id=i, body=document)\n",
    "    print(f\"N-gram Chunk {i} 인덱싱 완료\")\n",
    "\n",
    "es.indices.refresh(index=ngram_index_name)\n",
    "print(f\"✅ N-gram 인덱스에 {len(docs)}개의 chunk가 인덱싱되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram BM25 검색 함수와 성능 비교\n",
    "def ngram_bm25_search(query, top_k=3):\n",
    "    \"\"\"N-gram BM25 검색\"\"\"\n",
    "    search_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": {\n",
    "                    \"query\": query,\n",
    "                    \"operator\": \"or\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"content\", \"chunk_id\"],\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"content\": {\n",
    "                    \"fragment_size\": 150,\n",
    "                    \"number_of_fragments\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=ngram_index_name, body=search_body)\n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result = {\n",
    "            'chunk_id': hit['_source']['chunk_id'],\n",
    "            'score': hit['_score'],\n",
    "            'content': hit['_source']['content'],\n",
    "            'highlight': hit.get('highlight', {}).get('content', [])\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# Standard vs N-gram 분석기 성능 비교\n",
    "print(\"🔍 Standard vs N-gram 분석기 성능 비교\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_queries = [\"서울든든급식\", \"식재료 주문\", \"어린이집\", \"급식\", \"든든\"]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n📝 검색어: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Standard 분석기 결과\n",
    "    print(\"🔹 Standard 분석기:\")\n",
    "    standard_results = bm25_search(query, top_k=2)\n",
    "    if standard_results:\n",
    "        for i, result in enumerate(standard_results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"  검색 결과 없음\")\n",
    "    \n",
    "    # N-gram 분석기 결과\n",
    "    print(\"🔹 N-gram 분석기:\")\n",
    "    ngram_results = ngram_bm25_search(query, top_k=2)\n",
    "    if ngram_results:\n",
    "        for i, result in enumerate(ngram_results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"  검색 결과 없음\")\n",
    "    \n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 분석 결과 비교\n",
    "print(\"\\n🔍 토크나이저 분석 결과 비교\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def analyze_text(text, analyzer_name, index_name):\n",
    "    \"\"\"텍스트를 분석하여 토큰 결과를 반환\"\"\"\n",
    "    try:\n",
    "        response = es.indices.analyze(\n",
    "            index=index_name,\n",
    "            body={\n",
    "                \"analyzer\": analyzer_name,\n",
    "                \"text\": text\n",
    "            }\n",
    "        )\n",
    "        tokens = [token['token'] for token in response['tokens']]\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"분석 실패: {e}\")\n",
    "        return []\n",
    "\n",
    "# 분석할 텍스트 샘플\n",
    "sample_texts = [\n",
    "    \"서울든든급식\",\n",
    "    \"식재료 주문 방법\",\n",
    "    \"어린이집 대상\",\n",
    "    \"든든급식 개편사항\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    print(f\"\\n📝 분석 텍스트: '{text}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Standard 분석기 결과\n",
    "    standard_tokens = analyze_text(text, \"standard\", index_name)\n",
    "    print(f\"🔹 Standard: {standard_tokens}\")\n",
    "    \n",
    "    # N-gram 분석기 결과\n",
    "    ngram_tokens = analyze_text(text, \"korean_ngram_analyzer\", ngram_index_name)\n",
    "    print(f\"🔹 N-gram:   {ngram_tokens[:10]}{'...' if len(ngram_tokens) > 10 else ''}\")  # 처음 10개만 표시\n",
    "    \n",
    "    # 차이점 분석\n",
    "    print(f\"💡 토큰 개수: Standard({len(standard_tokens)}) vs N-gram({len(ngram_tokens)})\")\n",
    "    \n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nori 플러그인 설치 방법 안내\n",
    "print(\"\\n📋 Nori 플러그인 설치 방법\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Nori는 Elasticsearch의 공식 한국어 분석 플러그인입니다.\n",
    "현재 Docker 환경에서 Nori를 설치하려면 다음 방법을 사용하세요:\n",
    "\n",
    "🛠️ 방법 1: Docker Compose 수정\n",
    "docker-compose.yaml 파일을 다음과 같이 수정:\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  elasticsearch:\n",
    "    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.2\n",
    "    container_name: elasticsearch-local\n",
    "    environment:\n",
    "      - discovery.type=single-node\n",
    "      - ES_JAVA_OPTS=-Xms1g -Xmx1g\n",
    "      - xpack.security.enabled=false\n",
    "      - xpack.security.http.ssl.enabled=false\n",
    "    ulimits:\n",
    "      memlock:\n",
    "        soft: -1\n",
    "        hard: -1\n",
    "    volumes:\n",
    "      - esdata:/usr/share/elasticsearch/data\n",
    "    ports:\n",
    "     - \"9200:9200\"\n",
    "     - \"9300:9300\"\n",
    "    command: >\n",
    "      /bin/bash -c \"\n",
    "      elasticsearch-plugin install analysis-nori &&\n",
    "      /usr/local/bin/docker-entrypoint.sh eswrapper\n",
    "      \"\n",
    "```\n",
    "\n",
    "🛠️ 방법 2: 실행 중인 컨테이너에 설치\n",
    "```bash\n",
    "# 컨테이너에 접속\n",
    "docker exec -it elasticsearch-local bash\n",
    "\n",
    "# Nori 플러그인 설치\n",
    "elasticsearch-plugin install analysis-nori\n",
    "\n",
    "# Elasticsearch 재시작\n",
    "exit\n",
    "docker-compose restart elasticsearch\n",
    "```\n",
    "\n",
    "🛠️ 방법 3: 새로운 이미지 빌드\n",
    "Dockerfile을 만들어 Nori가 포함된 이미지 생성\n",
    "\n",
    "⚠️ 현재 상황에서는 N-gram 방식으로도 충분히 한글 BM25 성능을 향상시킬 수 있습니다!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n💡 N-gram vs Nori 비교\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\"\"\n",
    "✅ N-gram 방식 (현재 구현):\n",
    "- 설치 불필요, 즉시 사용 가능\n",
    "- 부분 매칭에 강함 (\"급식\" 검색으로 \"든든급식\" 찾기)\n",
    "- 2-3글자 단위로 분리하여 검색 범위 확장\n",
    "- 노이즈가 많을 수 있음\n",
    "\n",
    "✅ Nori 방식 (권장):\n",
    "- 형태소 분석으로 의미 단위 분리\n",
    "- 불용어 제거로 검색 정확도 향상\n",
    "- 복합어 분해 (예: \"든든급식\" → \"든든\" + \"급식\")\n",
    "- 더 정교한 한글 검색 지원\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🎯 결론: N-gram으로도 기본 대비 큰 성능 향상을 기대할 수 있습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 최적화 인덱스에 문서 인덱싱\n",
    "print(\"한글 최적화 인덱스에 문서 인덱싱 중...\")\n",
    "for i, doc in enumerate(docs):\n",
    "    document = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"chunk_id\": i\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 한글 최적화 인덱스에 문서 인덱싱\n",
    "        response = es.index(index=korean_index_name, id=i, body=document)\n",
    "        print(f\"한글 최적화 Chunk {i} 인덱싱 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"Chunk {i} 인덱싱 실패: {e}\")\n",
    "        break\n",
    "\n",
    "# 한글 최적화 인덱스 새로고침\n",
    "try:\n",
    "    es.indices.refresh(index=korean_index_name)\n",
    "    print(f\"✅ 한글 최적화 인덱스에 총 {len(docs)}개의 chunk가 인덱싱되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 인덱스 새로고침 실패: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 최적화 BM25 검색 함수\n",
    "def korean_bm25_search(query, top_k=3):\n",
    "    \"\"\"\n",
    "    한글 최적화 BM25 알고리즘을 사용하여 문서 검색\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 쿼리\n",
    "        top_k (int): 반환할 상위 결과 개수\n",
    "    \n",
    "    Returns:\n",
    "        list: 검색 결과와 점수\n",
    "    \"\"\"\n",
    "    search_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": {\n",
    "                    \"query\": query,\n",
    "                    \"operator\": \"or\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"content\", \"chunk_id\"],\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"content\": {\n",
    "                    \"fragment_size\": 150,\n",
    "                    \"number_of_fragments\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 한글 최적화 인덱스에서 검색 실행\n",
    "        response = es.search(index=korean_index_name, body=search_body)\n",
    "        \n",
    "        results = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            result = {\n",
    "                'chunk_id': hit['_source']['chunk_id'],\n",
    "                'score': hit['_score'],\n",
    "                'content': hit['_source']['content'],\n",
    "                'highlight': hit.get('highlight', {}).get('content', [])\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"검색 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "# Standard vs Nori 분석기 성능 비교\n",
    "print(\"🔍 Standard vs Nori 분석기 성능 비교\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_queries = [\n",
    "    \"서울든든급식\",\n",
    "    \"식재료 주문\", \n",
    "    \"어린이집\",\n",
    "    \"급식 개편사항\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n📝 검색어: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Standard 분석기 결과\n",
    "    print(\"🔹 Standard 분석기 (기본):\")\n",
    "    standard_results = bm25_search(query, top_k=3)\n",
    "    if standard_results:\n",
    "        for i, result in enumerate(standard_results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"  검색 결과 없음\")\n",
    "    \n",
    "    # Nori 분석기 결과 (한글 최적화)\n",
    "    print(\"🔹 Nori 분석기 (한글 최적화):\")\n",
    "    korean_results = korean_bm25_search(query, top_k=3)\n",
    "    if korean_results:\n",
    "        for i, result in enumerate(korean_results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"  검색 결과 없음\")\n",
    "    \n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 분석 결과 비교\n",
    "print(\"\\n🔍 토크나이저 분석 결과 비교\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 분석할 텍스트 샘플\n",
    "sample_texts = [\n",
    "    \"서울든든급식\",\n",
    "    \"식재료 주문 방법\",\n",
    "    \"어린이집 대상 신규 회원가입\",\n",
    "    \"든든급식 개편사항 안내\"\n",
    "]\n",
    "\n",
    "def analyze_text(text, analyzer_name, index_name):\n",
    "    \"\"\"텍스트를 분석하여 토큰 결과를 반환\"\"\"\n",
    "    try:\n",
    "        response = es.indices.analyze(\n",
    "            index=index_name,\n",
    "            body={\n",
    "                \"analyzer\": analyzer_name,\n",
    "                \"text\": text\n",
    "            }\n",
    "        )\n",
    "        tokens = [token['token'] for token in response['tokens']]\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"분석 실패: {e}\")\n",
    "        return []\n",
    "\n",
    "for text in sample_texts:\n",
    "    print(f\"\\n📝 분석 텍스트: '{text}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Standard 분석기 결과\n",
    "    standard_tokens = analyze_text(text, \"standard\", index_name)\n",
    "    print(f\"🔹 Standard: {standard_tokens}\")\n",
    "    \n",
    "    # Nori 분석기 결과\n",
    "    korean_tokens = analyze_text(text, \"nori_analyzer\", korean_index_name)\n",
    "    print(f\"🔹 Nori:     {korean_tokens}\")\n",
    "    \n",
    "    # 차이점 분석\n",
    "    if len(korean_tokens) != len(standard_tokens):\n",
    "        print(f\"💡 토큰 개수 차이: Standard({len(standard_tokens)}) vs Nori({len(korean_tokens)})\")\n",
    "    \n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram 토크나이저를 사용한 대안 방법 (Nori가 없는 경우)\n",
    "ngram_index_name = \"bm25_ngram_documents\"\n",
    "\n",
    "# N-gram 기반 인덱스 매핑 정의\n",
    "ngram_mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"similarity\": {\n",
    "                \"ngram_bm25_similarity\": {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,\n",
    "                    \"b\": 0.75\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"korean_ngram_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"keyword\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"korean_ngram_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"korean_ngram_filter\": {\n",
    "                    \"type\": \"ngram\",\n",
    "                    \"min_gram\": 2,\n",
    "                    \"max_gram\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"ngram_bm25_similarity\",\n",
    "                \"analyzer\": \"korean_ngram_analyzer\",\n",
    "                \"search_analyzer\": \"korean_ngram_analyzer\"\n",
    "            },\n",
    "            \"chunk_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# N-gram 인덱스 생성\n",
    "if es.indices.exists(index=ngram_index_name):\n",
    "    es.indices.delete(index=ngram_index_name)\n",
    "    print(f\"기존 N-gram 인덱스 '{ngram_index_name}'를 삭제했습니다.\")\n",
    "\n",
    "es.indices.create(index=ngram_index_name, body=ngram_mapping)\n",
    "print(f\"✅ N-gram 인덱스 '{ngram_index_name}'를 생성했습니다.\")\n",
    "\n",
    "# N-gram 인덱스에 문서 인덱싱\n",
    "for i, doc in enumerate(docs):\n",
    "    document = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"chunk_id\": i\n",
    "    }\n",
    "    es.index(index=ngram_index_name, id=i, body=document)\n",
    "\n",
    "es.indices.refresh(index=ngram_index_name)\n",
    "print(f\"✅ N-gram 인덱스에 {len(docs)}개의 chunk가 인덱싱되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3가지 토크나이저 성능 종합 비교\n",
    "def ngram_bm25_search(query, top_k=3):\n",
    "    \"\"\"N-gram BM25 검색\"\"\"\n",
    "    search_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": {\n",
    "                    \"query\": query,\n",
    "                    \"operator\": \"or\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"content\", \"chunk_id\"]\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=ngram_index_name, body=search_body)\n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result = {\n",
    "            'chunk_id': hit['_source']['chunk_id'],\n",
    "            'score': hit['_score'],\n",
    "            'content': hit['_source']['content']\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "print(\"\\n🏆 3가지 토크나이저 성능 종합 비교\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_queries = [\n",
    "    \"서울든든급식\",\n",
    "    \"급식 개편\",\n",
    "    \"식재료 주문\",\n",
    "    \"어린이집 회원가입\"\n",
    "]\n",
    "\n",
    "for query in comparison_queries:\n",
    "    print(f\"\\n🔍 검색어: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Standard 분석기\n",
    "    print(\"1️⃣ Standard 분석기 (기본):\")\n",
    "    standard_results = bm25_search(query, top_k=2)\n",
    "    if standard_results:\n",
    "        for i, result in enumerate(standard_results, 1):\n",
    "            print(f\"   [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "            print(f\"       내용: {result['content'][:80]}...\")\n",
    "    else:\n",
    "        print(\"   검색 결과 없음\")\n",
    "    \n",
    "    # 2. Nori 분석기 (한글 형태소 분석)\n",
    "    print(\"\\\\n2️⃣ Nori 분석기 (한글 형태소 분석):\")\n",
    "    korean_results = korean_bm25_search(query, top_k=2)\n",
    "    if korean_results:\n",
    "        for i, result in enumerate(korean_results, 1):\n",
    "            print(f\"   [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "            print(f\"       내용: {result['content'][:80]}...\")\n",
    "    else:\n",
    "        print(\"   검색 결과 없음 (Nori 플러그인 미설치)\")\n",
    "    \n",
    "    # 3. N-gram 분석기\n",
    "    print(\"\\\\n3️⃣ N-gram 분석기 (2-3글자 단위):\")\n",
    "    ngram_results = ngram_bm25_search(query, top_k=2)\n",
    "    if ngram_results:\n",
    "        for i, result in enumerate(ngram_results, 1):\n",
    "            print(f\"   [{i}] Chunk {result['chunk_id']} - 점수: {result['score']:.4f}\")\n",
    "            print(f\"       내용: {result['content'][:80]}...\")\n",
    "    else:\n",
    "        print(\"   검색 결과 없음\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elastic-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
