{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ë“ ë“ íˆ ë¨¹ê³ \n",
      "íŠ¼íŠ¼íˆ í¬ëŠ”\n",
      "ì„œìš¸ë“ ë“ ê¸‰ì‹# SEâ™¡UL M! SOUL# ì„œìš¸íŠ¹ë³„ì‹œ# < ì§„í–‰ ìˆœì„œ >| ë‚´ ìš© | ì§„ í–‰ |\n",
      "| --- | --- |\n",
      "| ë“ ë“ ê¸‰ì‹ ê°œí¸ì‚¬í•­ ì•ˆë‚´(10\n"
     ]
    }
   ],
   "source": [
    "with open(\"9f9e37a6-55f4-43c4-8285-b4b976f5dd4b.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(content[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap =10,\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = text_splitter.create_documents([content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearchì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "Elasticsearch ë²„ì „: 9.0.2\n"
     ]
    }
   ],
   "source": [
    "# Elasticsearch BM25 êµ¬í˜„\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "# Elasticsearch í´ë¼ì´ì–¸íŠ¸ ì—°ê²° (9.x ë²„ì „ í˜¸í™˜)\n",
    "es = Elasticsearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "# ì—°ê²° í™•ì¸\n",
    "try:\n",
    "    if es.ping():\n",
    "        print(\"Elasticsearchì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶œë ¥\n",
    "        info = es.info()\n",
    "        print(f\"Elasticsearch ë²„ì „: {info['version']['number']}\")\n",
    "    else:\n",
    "        print(\"Elasticsearch ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"Elasticsearch ì—°ê²° ì˜¤ë¥˜: {e}\")\n",
    "    print(\"Docker Composeë¡œ Elasticsearchê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ ì¸ë±ìŠ¤ 'bm25_documents'ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\n",
      "ì¸ë±ìŠ¤ 'bm25_documents'ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì¸ë±ìŠ¤ ìƒì„± (BM25 ì„¤ì • í¬í•¨)\n",
    "index_name = \"bm25_documents\"\n",
    "\n",
    "# ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜ (BM25 íŒŒë¼ë¯¸í„° ì„¤ì •)\n",
    "mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"similarity\": {\n",
    "                \"bm25_similarity\": {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,    # ê¸°ë³¸ê°’\n",
    "                    \"b\": 0.75     # ê¸°ë³¸ê°’\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"bm25_similarity\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            },\n",
    "            \"chunk_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œ\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"ê¸°ì¡´ ì¸ë±ìŠ¤ '{index_name}'ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ì¸ë±ìŠ¤ ìƒì„±\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "print(f\"ì¸ë±ìŠ¤ '{index_name}'ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 1 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 2 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 3 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 4 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 5 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 6 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 7 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 8 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 9 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 10 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 11 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 12 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 13 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 14 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 15 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 16 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 17 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 18 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 19 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 20 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 21 ì¸ë±ì‹± ì™„ë£Œ\n",
      "Chunk 22 ì¸ë±ì‹± ì™„ë£Œ\n",
      "ì´ 23ê°œì˜ chunkê°€ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ chunkë“¤ì„ Elasticsearchì— ì¸ë±ì‹±\n",
    "for i, doc in enumerate(docs):\n",
    "    document = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"chunk_id\": i\n",
    "    }\n",
    "    \n",
    "    # ë¬¸ì„œ ì¸ë±ì‹±\n",
    "    response = es.index(index=index_name, id=i, body=document)\n",
    "    print(f\"Chunk {i} ì¸ë±ì‹± ì™„ë£Œ\")\n",
    "\n",
    "# ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨ (ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡)\n",
    "es.indices.refresh(index=index_name)\n",
    "print(f\"ì´ {len(docs)}ê°œì˜ chunkê°€ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ìƒ‰ì–´: 'ì„œìš¸ë“ ë“ ê¸‰ì‹'\n",
      "ê²€ìƒ‰ ê²°ê³¼ (3ê°œ):\n",
      "==================================================\n",
      "[ê²°ê³¼ 1] Chunk ID: 7, BM25 ì ìˆ˜: 2.8021\n",
      "ë‚´ìš©: 11ì›” 20ì¼(ìˆ˜)ë¶€í„° '24ë…„ 12ì›”ë¶„ ì‹ì¬ë£Œ ì£¼ë¬¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì„±ë™êµ¬ ì–´ë¦°ì´ì§‘ ëŒ€ìƒ ì‹ ê·œ íšŒì›ê°€ì… ì•ˆë‚´ >\n",
      "\n",
      "íšŒì›ê°€ì… í›„ 12ì›” 20ì¼(ê¸ˆ) ì¼ê´„ ìŠ¹ì¸ë  ì˜ˆì •ì´ë©°,\n",
      "\n",
      "ìŠ¹ì¸ì™„ë£Œ ì‹œì—ëŠ” [ìŠ¹ì¸ì™„ë£Œ] ë¬¸ìê°€ ë°œì†¡ë©ë‹ˆë‹¤.\n",
      "\n",
      "12ì›” 20ì¼(ê¸ˆ)ë¶€í„° '25ë…„ 1ì›”ë¶„ ì‹ì¬ë£Œ ì£¼ë¬¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "ë§¤ì›” 20ì¼ê¹Œì§€ ì‹ ì²­ ì‹œ ìµì›” 1ì¼ë¶„ë¶€í„° ì£¼ë¬¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "- ì„œ...\n",
      "í•˜ì´ë¼ì´íŠ¸: <em>ì„œìš¸ë“ ë“ ê¸‰ì‹</em> ì‹ ì²­ ë°”ë¡œê°€ê¸°\n",
      "\n",
      "# 1-3....\n",
      "------------------------------\n",
      "[ê²°ê³¼ 2] Chunk ID: 0, BM25 ì ìˆ˜: 2.6964\n",
      "ë‚´ìš©: # ë“ ë“ íˆ ë¨¹ê³ \n",
      "íŠ¼íŠ¼íˆ í¬ëŠ”\n",
      "ì„œìš¸ë“ ë“ ê¸‰ì‹# SEâ™¡UL M! SOUL# ì„œìš¸íŠ¹ë³„ì‹œ# < ì§„í–‰ ìˆœì„œ >| ë‚´ ìš© | ì§„ í–‰ |\n",
      "| --- | --- |\n",
      "| ë“ ë“ ê¸‰ì‹ ê°œí¸ì‚¬í•­ ì•ˆë‚´(10ë¶„) | ì„œìš¸ì‹œ ì¹œí™˜ê²½ê¸‰ì‹ê³¼ ê³µê³µê¸‰ì‹íŒ€ì¥ |\n",
      "| Q&A(10ë¶„) |  |\n",
      "| ë“ ë“ ê¸‰ì‹ ì›¹ì‡¼í•‘ëª° ì‚¬ìš©ë²• (30ë¶„) | ë‚˜ë¼ì›ì‹œìŠ¤í…œ |\n",
      "| Q&A(10ë¶„) |  |\n",
      "\n",
      "\n",
      "' \"\n",
      "ì„œìš¸ë“ ë“ ...\n",
      "í•˜ì´ë¼ì´íŠ¸: # ë“ ë“ íˆ ë¨¹ê³ \n",
      "íŠ¼íŠ¼íˆ í¬ëŠ”\n",
      "<em>ì„œìš¸ë“ ë“ ê¸‰ì‹</em># SEâ™¡UL M!...\n",
      "------------------------------\n",
      "[ê²°ê³¼ 3] Chunk ID: 6, BM25 ì ìˆ˜: 2.5700\n",
      "ë‚´ìš©: # 4. ì‹ì¬ë£Œ ì£¼ë¬¸ ë°©ë²•# 1 ì‹ì¬ë£Œì£¼ë¬¸ ì„œìš¸ë“ ë“ ê¸‰ì‹ ì›¹ì‡¼í•‘ëª° ì´ìš©# Â· ë°œì£¼ê¸°í•œ ë‚©í’ˆì¼ ê¸°ì¤€ 7ì¼ ì „ê¹Œì§€- x (ì€í‰Â·ì†¡íŒŒ) '24.12ì›” 2ì¼ë¶„ ì£¼ë¬¸ ì‹œ 11ì›” 20ì¼~ 25ì¼ê¹Œì§€ ì£¼ë¬¸ ê°€ëŠ¥\n",
      "- (ì„±ë™) '25. 1ì›” 2ì¼ë¶„ ì£¼ë¬¸ ì‹œ 12ì›” 20ì¼~26ì¼ê¹Œì§€ ì£¼ë¬¸ ê°€ëŠ¥\n",
      "- Â· ë°œì£¼ ì·¨ì†Œ ë° ë³€ê²½ ë‚©í’ˆì¼ ê¸°ì¤€ 3ì¼ ì „ê¹Œì§€\n",
      "- Â· í´ë ˆì„ ìš”ì²­ ì‹œì—ëŠ”, ë°°...\n",
      "í•˜ì´ë¼ì´íŠ¸: ì‹ì¬ë£Œ ì£¼ë¬¸ ë°©ë²•# 1 ì‹ì¬ë£Œì£¼ë¬¸ <em>ì„œìš¸ë“ ë“ ê¸‰ì‹</em> ì›¹ì‡¼í•‘ëª° ì´ìš©# Â· ë°œì£¼ê¸°í•œ ë‚©í’ˆì¼ ê¸°ì¤€ 7ì¼ ì „ê¹Œì§€- x (ì€í‰Â·ì†¡íŒŒ) '24.12ì›” 2ì¼ë¶„ ì£¼ë¬¸ ì‹œ 11ì›” 20ì¼~ 25ì¼ê¹Œì§€ ì£¼ë¬¸ ê°€ëŠ¥\n",
      "- (ì„±ë™) '25. 1ì›” 2ì¼ë¶„ ì£¼ë¬¸ ì‹œ 12ì›” 20ì¼~26ì¼ê¹Œ...\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# BM25 ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def bm25_search(query, top_k=3):\n",
    "    \"\"\"\n",
    "    BM25 ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    \n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰ ì¿¼ë¦¬\n",
    "        top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        list: ê²€ìƒ‰ ê²°ê³¼ì™€ ì ìˆ˜\n",
    "    \"\"\"\n",
    "    search_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": {\n",
    "                    \"query\": query,\n",
    "                    \"operator\": \"or\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"content\", \"chunk_id\"],\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"content\": {\n",
    "                    \"fragment_size\": 150,\n",
    "                    \"number_of_fragments\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ê²€ìƒ‰ ì‹¤í–‰\n",
    "    response = es.search(index=index_name, body=search_body)\n",
    "    \n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result = {\n",
    "            'chunk_id': hit['_source']['chunk_id'],\n",
    "            'score': hit['_score'],\n",
    "            'content': hit['_source']['content'],\n",
    "            'highlight': hit.get('highlight', {}).get('content', [])\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "test_query = \"ì„œìš¸ë“ ë“ ê¸‰ì‹\"\n",
    "search_results = bm25_search(test_query, top_k=3)\n",
    "\n",
    "print(f\"ê²€ìƒ‰ì–´: '{test_query}'\")\n",
    "print(f\"ê²€ìƒ‰ ê²°ê³¼ ({len(search_results)}ê°œ):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"[ê²°ê³¼ {i}] Chunk ID: {result['chunk_id']}, BM25 ì ìˆ˜: {result['score']:.4f}\")\n",
    "    print(f\"ë‚´ìš©: {result['content'][:200]}...\")\n",
    "    if result['highlight']:\n",
    "        print(f\"í•˜ì´ë¼ì´íŠ¸: {result['highlight'][0][:150]}...\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ë¡œ BM25 í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "\n",
      "ğŸ” ê²€ìƒ‰ì–´: 'ë””ì§€í„¸ ê¸°ìˆ '\n",
      "  [1] Chunk 0 - ì ìˆ˜: 3.1239\n",
      "      ë‚´ìš©: # ã€ŒAXë¸Œë¦¿ì§€ìœ„ì›íšŒã€ ì†Œê°œìë£Œ(ë²¤ì²˜ê¸°ì—…í˜‘íšŒ íšŒì›ì†Œí†µë³¸ë¶€, 2024.10.11.)\n",
      "\n",
      "ë°ì´í„°, ë„¤íŠ¸ì›Œí¬, AI ë“± ì²¨ë‹¨ ë””ì§€í„¸ ê¸°ìˆ  ë¶„ì•¼ ì„ ë„ë²¤ì²˜ì™€ ìœ ë§ ìŠ¤\n",
      "íƒ€íŠ¸ì—…ì´ í•¨ê»˜ ëª¨ì—¬ ë””ì§€...\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ” ê²€ìƒ‰ì–´: 'ìŠ¤íƒ€íŠ¸ì—…'\n",
      "  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ” ê²€ìƒ‰ì–´: 'AXë¸Œë¦¿ì§€ìœ„ì›íšŒ'\n",
      "  [1] Chunk 0 - ì ìˆ˜: 0.7727\n",
      "      ë‚´ìš©: # ã€ŒAXë¸Œë¦¿ì§€ìœ„ì›íšŒã€ ì†Œê°œìë£Œ(ë²¤ì²˜ê¸°ì—…í˜‘íšŒ íšŒì›ì†Œí†µë³¸ë¶€, 2024.10.11.)\n",
      "\n",
      "ë°ì´í„°, ë„¤íŠ¸ì›Œí¬, AI ë“± ì²¨ë‹¨ ë””ì§€í„¸ ê¸°ìˆ  ë¶„ì•¼ ì„ ë„ë²¤ì²˜ì™€ ìœ ë§ ìŠ¤\n",
      "íƒ€íŠ¸ì—…ì´ í•¨ê»˜ ëª¨ì—¬ ë””ì§€...\n",
      "  [2] Chunk 1 - ì ìˆ˜: 0.6386\n",
      "      ë‚´ìš©: # â–¡ ìš´ì˜ì¡°ì§- â—‹ ì˜ ì¥ : ì´ì£¼ì™„ ëŒ€í‘œ(ë©”ê°€ì¡´í´ë¼ìš°ë“œ)\n",
      "- â—‹ ìš´ì˜ìœ„ì› : 10ëª… ë‚´ì™¸(ì¶”ê°€ì„­ì™¸ì¤‘)\n",
      "- â—‹ ì‚¬ ë¬´ êµ­ : ë²¤ì²˜ê¸°ì—…í˜‘íšŒ íšŒì›ì†Œí†µë³¸ë¶€\n",
      "# â–¡ ì£¼ìš”í™œë™- 1. (ì •...\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ” ê²€ìƒ‰ì–´: 'ë„¤íŠ¸ì›Œí¬ AI'\n",
      "  [1] Chunk 0 - ì ìˆ˜: 1.7146\n",
      "      ë‚´ìš©: # ã€ŒAXë¸Œë¦¿ì§€ìœ„ì›íšŒã€ ì†Œê°œìë£Œ(ë²¤ì²˜ê¸°ì—…í˜‘íšŒ íšŒì›ì†Œí†µë³¸ë¶€, 2024.10.11.)\n",
      "\n",
      "ë°ì´í„°, ë„¤íŠ¸ì›Œí¬, AI ë“± ì²¨ë‹¨ ë””ì§€í„¸ ê¸°ìˆ  ë¶„ì•¼ ì„ ë„ë²¤ì²˜ì™€ ìœ ë§ ìŠ¤\n",
      "íƒ€íŠ¸ì—…ì´ í•¨ê»˜ ëª¨ì—¬ ë””ì§€...\n",
      "  [2] Chunk 1 - ì ìˆ˜: 1.2759\n",
      "      ë‚´ìš©: # â–¡ ìš´ì˜ì¡°ì§- â—‹ ì˜ ì¥ : ì´ì£¼ì™„ ëŒ€í‘œ(ë©”ê°€ì¡´í´ë¼ìš°ë“œ)\n",
      "- â—‹ ìš´ì˜ìœ„ì› : 10ëª… ë‚´ì™¸(ì¶”ê°€ì„­ì™¸ì¤‘)\n",
      "- â—‹ ì‚¬ ë¬´ êµ­ : ë²¤ì²˜ê¸°ì—…í˜‘íšŒ íšŒì›ì†Œí†µë³¸ë¶€\n",
      "# â–¡ ì£¼ìš”í™œë™- 1. (ì •...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ê°€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ - ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ í…ŒìŠ¤íŠ¸\n",
    "test_queries = [\n",
    "    \"ë””ì§€í„¸ ê¸°ìˆ \",\n",
    "    \"ìŠ¤íƒ€íŠ¸ì—…\",\n",
    "    \"AXë¸Œë¦¿ì§€ìœ„ì›íšŒ\",\n",
    "    \"ë„¤íŠ¸ì›Œí¬ AI\"\n",
    "]\n",
    "\n",
    "print(\"ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ë¡œ BM25 í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nğŸ” ê²€ìƒ‰ì–´: '{query}'\")\n",
    "    results = bm25_search(query, top_k=2)\n",
    "    \n",
    "    if results:\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "            print(f\"      ë‚´ìš©: {result['content'][:100]}...\")\n",
    "    else:\n",
    "        print(\"  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: 'ë²¤ì²˜ê¸°ì—…í˜‘íšŒ'\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š ê¸°ë³¸ BM25 íŒŒë¼ë¯¸í„° (k1=1.2, b=0.75):\n",
      "  [1] Chunk 1 - ì ìˆ˜: 0.4632\n",
      "  [2] Chunk 0 - ì ìˆ˜: 0.3976\n",
      "\n",
      "ğŸ“Š ì¡°ì •ëœ BM25 íŒŒë¼ë¯¸í„° (k1=2.0, b=0.5):\n",
      "ì¸ë±ìŠ¤ 'bm25_documents' ë‹«ëŠ” ì¤‘...\n",
      "ì¸ë±ìŠ¤ 'bm25_documents' ë‹¤ì‹œ ì—¬ëŠ” ì¤‘...\n",
      "âœ… BM25 íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: k1=2.0, b=0.5\n",
      "  [1] Chunk 1 - ì ìˆ˜: 0.5085\n",
      "  [2] Chunk 0 - ì ìˆ˜: 0.3894\n",
      "\n",
      "ğŸ“Š ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µêµ¬:\n",
      "ì¸ë±ìŠ¤ 'bm25_documents' ë‹«ëŠ” ì¤‘...\n",
      "ì¸ë±ìŠ¤ 'bm25_documents' ë‹¤ì‹œ ì—¬ëŠ” ì¤‘...\n",
      "âœ… BM25 íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: k1=1.2, b=0.75\n"
     ]
    }
   ],
   "source": [
    "# BM25 íŒŒë¼ë¯¸í„° ì¡°ì • í…ŒìŠ¤íŠ¸ (ìˆ˜ì •ëœ ë²„ì „)\n",
    "def update_bm25_parameters(k1=1.2, b=0.75):\n",
    "    \"\"\"\n",
    "    BM25 íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜ (ì¸ë±ìŠ¤ ë‹«ê¸°/ì—´ê¸° ë°©ì‹)\n",
    "    \n",
    "    Args:\n",
    "        k1 (float): term frequency ì¡°ì • íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 1.2)\n",
    "        b (float): document length normalization íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 0.75)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. ì¸ë±ìŠ¤ ë‹«ê¸°\n",
    "        print(f\"ì¸ë±ìŠ¤ '{index_name}' ë‹«ëŠ” ì¤‘...\")\n",
    "        es.indices.close(index=index_name)\n",
    "        \n",
    "        # 2. ì„¤ì • ì—…ë°ì´íŠ¸\n",
    "        settings = {\n",
    "            \"index\": {\n",
    "                \"similarity\": {\n",
    "                    \"bm25_similarity\": {\n",
    "                        \"type\": \"BM25\",\n",
    "                        \"k1\": k1,\n",
    "                        \"b\": b\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        es.indices.put_settings(index=index_name, body=settings)\n",
    "        \n",
    "        # 3. ì¸ë±ìŠ¤ ë‹¤ì‹œ ì—´ê¸°\n",
    "        print(f\"ì¸ë±ìŠ¤ '{index_name}' ë‹¤ì‹œ ì—¬ëŠ” ì¤‘...\")\n",
    "        es.indices.open(index=index_name)\n",
    "        \n",
    "        # 4. ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨\n",
    "        es.indices.refresh(index=index_name)\n",
    "        \n",
    "        print(f\"âœ… BM25 íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: k1={k1}, b={b}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ BM25 íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        # ì¸ë±ìŠ¤ê°€ ë‹«í˜€ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë‹¤ì‹œ ì—´ì–´ì£¼ê¸°\n",
    "        try:\n",
    "            es.indices.open(index=index_name)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# ì„œë¡œ ë‹¤ë¥¸ BM25 íŒŒë¼ë¯¸í„°ë¡œ í…ŒìŠ¤íŠ¸\n",
    "query = \"ë²¤ì²˜ê¸°ì—…í˜‘íšŒ\"\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{query}'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ê¸°ë³¸ íŒŒë¼ë¯¸í„° (k1=1.2, b=0.75)\n",
    "print(\"\\nğŸ“Š ê¸°ë³¸ BM25 íŒŒë¼ë¯¸í„° (k1=1.2, b=0.75):\")\n",
    "results = bm25_search(query, top_k=2)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ì¡°ì • 1 (k1=2.0, b=0.5)\n",
    "print(\"\\nğŸ“Š ì¡°ì •ëœ BM25 íŒŒë¼ë¯¸í„° (k1=2.0, b=0.5):\")\n",
    "update_bm25_parameters(k1=2.0, b=0.5)\n",
    "results = bm25_search(query, top_k=2)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "\n",
    "# ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µêµ¬\n",
    "print(\"\\nğŸ“Š ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µêµ¬:\")\n",
    "update_bm25_parameters(k1=1.2, b=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ì¸ë±ì‹±ëœ ë¬¸ì„œ ë‚´ìš© í™•ì¸:\n",
      "==================================================\n",
      "\n",
      "[Chunk 0]\n",
      "ê¸¸ì´: 470 ë¬¸ì\n",
      "ë‚´ìš©: # ã€ŒAXë¸Œë¦¿ì§€ìœ„ì›íšŒã€ ì†Œê°œìë£Œ(ë²¤ì²˜ê¸°ì—…í˜‘íšŒ íšŒì›ì†Œí†µë³¸ë¶€, 2024.10.11.)\n",
      "\n",
      "ë°ì´í„°, ë„¤íŠ¸ì›Œí¬, AI ë“± ì²¨ë‹¨ ë””ì§€í„¸ ê¸°ìˆ  ë¶„ì•¼ ì„ ë„ë²¤ì²˜ì™€ ìœ ë§ ìŠ¤\n",
      "íƒ€íŠ¸ì—…ì´ í•¨ê»˜ ëª¨ì—¬ ë””ì§€í„¸ì‹œëŒ€ì˜ ê²½ìŸìš°ìœ„ í™•ë³´ë¥¼ ìœ„í•´ í•„ìš”í•œ ì „ëµ\n",
      "ì  í˜‘ë ¥ì„ ì¶”êµ¬í•˜ê³  ê·œì œ ì •ë¹„ì™€ ì •ì±…ì  ì§€ì›ì„ ...\n",
      "------------------------------\n",
      "\n",
      "[Chunk 1]\n",
      "ê¸¸ì´: 997 ë¬¸ì\n",
      "ë‚´ìš©: # â–¡ ìš´ì˜ì¡°ì§- â—‹ ì˜ ì¥ : ì´ì£¼ì™„ ëŒ€í‘œ(ë©”ê°€ì¡´í´ë¼ìš°ë“œ)\n",
      "- â—‹ ìš´ì˜ìœ„ì› : 10ëª… ë‚´ì™¸(ì¶”ê°€ì„­ì™¸ì¤‘)\n",
      "- â—‹ ì‚¬ ë¬´ êµ­ : ë²¤ì²˜ê¸°ì—…í˜‘íšŒ íšŒì›ì†Œí†µë³¸ë¶€\n",
      "# â–¡ ì£¼ìš”í™œë™- 1. (ì •ê¸°í¬ëŸ¼) êµ­ë‚´ì™¸ ì—…ê³„ë™í–¥ ë° ì„ ë„ë²¤ì²˜ ê¸°ì—…ì‚¬ë¡€ ì¤‘ì‹¬ì˜ ì •ë³´êµë¥˜ ëª©\n",
      "- ì ì˜ ì •ê¸°í¬ëŸ¼ 2íšŒ(...\n",
      "------------------------------\n",
      "\n",
      "[Chunk 2]\n",
      "ê¸¸ì´: 760 ë¬¸ì\n",
      "ë‚´ìš©: | <ìš´ì˜ìœ„ì›> â—‹ ì•„ì‹œì•„ íƒœí‰ì–‘ ê³ ì„±ì¥ ê¸°ì—… 3ë…„ ì—°ì† ì„ ì • â—‹ ì•„ì‹œì•„' 30ì„¸ ì´í•˜ ë¦¬ë” 30ì¸ ì„ ì • (í¬ë¸ŒìŠ¤) â—‹ ì§ê°€ë§¹ì  200ê°œ â†‘ í•´ì™¸ 100í˜¸ì  ëŒíŒŒ â—‹ ì˜¬í•´ ë§¤ì¥ ìˆ˜ 500í˜¸ì  ëŒíŒŒ ì˜ˆì • â—‹ ëˆ„ì  íˆ¬ì ìœ ì¹˜ 600ì–µì› |\n",
      "| 4 | (ì£¼)ë¤¼íŠ¼ í…Œí¬ë†€ë¡œ ì§€...\n",
      "------------------------------\n",
      "\n",
      "[Chunk 3]\n",
      "ê¸¸ì´: 856 ë¬¸ì\n",
      "ë‚´ìš©: | ìˆœì„œ | ì†Œì†/ì„±ëª… | ì‚¬ì§„ | ì£¼ìš”ì•½ë ¥ |\n",
      "| --- | --- | --- | --- |\n",
      "| 8 | (ì£¼)ìŠ¤íŒŒì´ì–´ í…Œí¬ë†€ë¡œì§€ ê°•êµ°í™” ëŒ€í‘œì´ì‚¬ | ![image](/image/placeholder)\n",
      " | <ìš´ì˜ìœ„ì›> â—‹ (ì£¼)ìŠ¤íŒŒì´ì–´í…Œí¬ë†€ë¡œì§€, (ì£¼)ìŠ¤í…Œì´ì§€7 ëŒ€í‘œ...\n",
      "------------------------------\n",
      "\n",
      "ğŸ“Š í†µê³„:\n",
      "- ì´ ë¬¸ì„œ ìˆ˜: 4\n",
      "- í‰ê·  ë¬¸ì„œ ê¸¸ì´: 771 ë¬¸ì\n",
      "- ìµœë‹¨ ë¬¸ì„œ: 470 ë¬¸ì\n",
      "- ìµœì¥ ë¬¸ì„œ: 997 ë¬¸ì\n"
     ]
    }
   ],
   "source": [
    "# ì¸ë±ì‹±ëœ ë¬¸ì„œë“¤ í™•ì¸\n",
    "print(\"ğŸ“„ ì¸ë±ì‹±ëœ ë¬¸ì„œ ë‚´ìš© í™•ì¸:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n[Chunk {i}]\")\n",
    "    print(f\"ê¸¸ì´: {len(doc.page_content)} ë¬¸ì\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content[:150]}...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# í†µê³„ ì •ë³´\n",
    "print(f\"\\nğŸ“Š í†µê³„:\")\n",
    "print(f\"- ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}\")\n",
    "print(f\"- í‰ê·  ë¬¸ì„œ ê¸¸ì´: {sum(len(doc.page_content) for doc in docs) / len(docs):.0f} ë¬¸ì\")\n",
    "print(f\"- ìµœë‹¨ ë¬¸ì„œ: {min(len(doc.page_content) for doc in docs)} ë¬¸ì\")\n",
    "print(f\"- ìµœì¥ ë¬¸ì„œ: {max(len(doc.page_content) for doc in docs)} ë¬¸ì\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: BadRequestError(400, 'illegal_argument_exception', 'No enum constant org.apache.lucene.analysis.ko.POS.Tag.E')\n",
      "Nori í”ŒëŸ¬ê·¸ì¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í•œê¸€ ìµœì í™” BM25 ì¸ë±ìŠ¤ ìƒì„± (Nori ë¶„ì„ê¸° ì‚¬ìš©)\n",
    "korean_index_name = \"bm25_korean_documents\"\n",
    "\n",
    "# Nori ë¶„ì„ê¸°ê°€ í¬í•¨ëœ ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜\n",
    "korean_mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"similarity\": {\n",
    "                \"korean_bm25_similarity\": {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,\n",
    "                    \"b\": 0.75\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"nori_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"nori_tokenizer\",\n",
    "                    \"filter\": [\n",
    "                        \"nori_part_of_speech\",\n",
    "                        \"nori_readingform\",\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"nori_tokenizer\": {\n",
    "                    \"type\": \"nori_tokenizer\",\n",
    "                    \"decompound_mode\": \"mixed\"\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"nori_part_of_speech\": {\n",
    "                    \"type\": \"nori_part_of_speech\",\n",
    "                    \"stoptags\": [\n",
    "                        \"E\", \"IC\", \"J\", \"MAG\", \"MAJ\", \"MM\", \n",
    "                        \"SP\", \"SSC\", \"SSO\", \"SC\", \"SE\", \"XPN\", \"XSA\", \"XSN\", \"XSV\",\n",
    "                        \"UNA\", \"NA\", \"VSV\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"korean_bm25_similarity\",\n",
    "                \"analyzer\": \"nori_analyzer\",\n",
    "                \"search_analyzer\": \"nori_analyzer\"\n",
    "            },\n",
    "            \"chunk_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ê¸°ì¡´ í•œê¸€ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œ\n",
    "if es.indices.exists(index=korean_index_name):\n",
    "    es.indices.delete(index=korean_index_name)\n",
    "    print(f\"ê¸°ì¡´ í•œê¸€ ì¸ë±ìŠ¤ '{korean_index_name}'ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# í•œê¸€ ìµœì í™” ì¸ë±ìŠ¤ ìƒì„±\n",
    "try:\n",
    "    es.indices.create(index=korean_index_name, body=korean_mapping)\n",
    "    print(f\"âœ… í•œê¸€ ìµœì í™” ì¸ë±ìŠ¤ '{korean_index_name}'ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "    print(\"Nori í”ŒëŸ¬ê·¸ì¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram ë°©ì‹ìœ¼ë¡œ í•œê¸€ ìµœì í™” BM25 êµ¬í˜„ (Nori ëŒ€ì•ˆ)\n",
    "ngram_index_name = \"bm25_ngram_documents\"\n",
    "\n",
    "# N-gram ê¸°ë°˜ ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜\n",
    "ngram_mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"similarity\": {\n",
    "                \"ngram_bm25_similarity\": {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,\n",
    "                    \"b\": 0.75\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"korean_ngram_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"keyword\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"korean_ngram_filter\"\n",
    "                    ]\n",
    "                },\n",
    "                \"korean_search_analyzer\": {\n",
    "                    \"type\": \"custom\", \n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"korean_ngram_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"korean_ngram_filter\": {\n",
    "                    \"type\": \"ngram\",\n",
    "                    \"min_gram\": 2,\n",
    "                    \"max_gram\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"ngram_bm25_similarity\",\n",
    "                \"analyzer\": \"korean_ngram_analyzer\",\n",
    "                \"search_analyzer\": \"korean_search_analyzer\"\n",
    "            },\n",
    "            \"chunk_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# N-gram ì¸ë±ìŠ¤ ìƒì„±\n",
    "if es.indices.exists(index=ngram_index_name):\n",
    "    es.indices.delete(index=ngram_index_name)\n",
    "    print(f\"ê¸°ì¡´ N-gram ì¸ë±ìŠ¤ '{ngram_index_name}'ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "es.indices.create(index=ngram_index_name, body=ngram_mapping)\n",
    "print(f\"âœ… N-gram ì¸ë±ìŠ¤ '{ngram_index_name}'ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# N-gram ì¸ë±ìŠ¤ì— ë¬¸ì„œ ì¸ë±ì‹±\n",
    "print(\"N-gram ì¸ë±ìŠ¤ì— ë¬¸ì„œ ì¸ë±ì‹± ì¤‘...\")\n",
    "for i, doc in enumerate(docs):\n",
    "    document = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"chunk_id\": i\n",
    "    }\n",
    "    es.index(index=ngram_index_name, id=i, body=document)\n",
    "    print(f\"N-gram Chunk {i} ì¸ë±ì‹± ì™„ë£Œ\")\n",
    "\n",
    "es.indices.refresh(index=ngram_index_name)\n",
    "print(f\"âœ… N-gram ì¸ë±ìŠ¤ì— {len(docs)}ê°œì˜ chunkê°€ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram BM25 ê²€ìƒ‰ í•¨ìˆ˜ì™€ ì„±ëŠ¥ ë¹„êµ\n",
    "def ngram_bm25_search(query, top_k=3):\n",
    "    \"\"\"N-gram BM25 ê²€ìƒ‰\"\"\"\n",
    "    search_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": {\n",
    "                    \"query\": query,\n",
    "                    \"operator\": \"or\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"content\", \"chunk_id\"],\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"content\": {\n",
    "                    \"fragment_size\": 150,\n",
    "                    \"number_of_fragments\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=ngram_index_name, body=search_body)\n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result = {\n",
    "            'chunk_id': hit['_source']['chunk_id'],\n",
    "            'score': hit['_score'],\n",
    "            'content': hit['_source']['content'],\n",
    "            'highlight': hit.get('highlight', {}).get('content', [])\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# Standard vs N-gram ë¶„ì„ê¸° ì„±ëŠ¥ ë¹„êµ\n",
    "print(\"ğŸ” Standard vs N-gram ë¶„ì„ê¸° ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_queries = [\"ì„œìš¸ë“ ë“ ê¸‰ì‹\", \"ì‹ì¬ë£Œ ì£¼ë¬¸\", \"ì–´ë¦°ì´ì§‘\", \"ê¸‰ì‹\", \"ë“ ë“ \"]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nğŸ“ ê²€ìƒ‰ì–´: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Standard ë¶„ì„ê¸° ê²°ê³¼\n",
    "    print(\"ğŸ”¹ Standard ë¶„ì„ê¸°:\")\n",
    "    standard_results = bm25_search(query, top_k=2)\n",
    "    if standard_results:\n",
    "        for i, result in enumerate(standard_results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "    \n",
    "    # N-gram ë¶„ì„ê¸° ê²°ê³¼\n",
    "    print(\"ğŸ”¹ N-gram ë¶„ì„ê¸°:\")\n",
    "    ngram_results = ngram_bm25_search(query, top_k=2)\n",
    "    if ngram_results:\n",
    "        for i, result in enumerate(ngram_results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "    \n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë¶„ì„ ê²°ê³¼ ë¹„êµ\n",
    "print(\"\\nğŸ” í† í¬ë‚˜ì´ì € ë¶„ì„ ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def analyze_text(text, analyzer_name, index_name):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í† í° ê²°ê³¼ë¥¼ ë°˜í™˜\"\"\"\n",
    "    try:\n",
    "        response = es.indices.analyze(\n",
    "            index=index_name,\n",
    "            body={\n",
    "                \"analyzer\": analyzer_name,\n",
    "                \"text\": text\n",
    "            }\n",
    "        )\n",
    "        tokens = [token['token'] for token in response['tokens']]\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "# ë¶„ì„í•  í…ìŠ¤íŠ¸ ìƒ˜í”Œ\n",
    "sample_texts = [\n",
    "    \"ì„œìš¸ë“ ë“ ê¸‰ì‹\",\n",
    "    \"ì‹ì¬ë£Œ ì£¼ë¬¸ ë°©ë²•\",\n",
    "    \"ì–´ë¦°ì´ì§‘ ëŒ€ìƒ\",\n",
    "    \"ë“ ë“ ê¸‰ì‹ ê°œí¸ì‚¬í•­\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    print(f\"\\nğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸: '{text}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Standard ë¶„ì„ê¸° ê²°ê³¼\n",
    "    standard_tokens = analyze_text(text, \"standard\", index_name)\n",
    "    print(f\"ğŸ”¹ Standard: {standard_tokens}\")\n",
    "    \n",
    "    # N-gram ë¶„ì„ê¸° ê²°ê³¼\n",
    "    ngram_tokens = analyze_text(text, \"korean_ngram_analyzer\", ngram_index_name)\n",
    "    print(f\"ğŸ”¹ N-gram:   {ngram_tokens[:10]}{'...' if len(ngram_tokens) > 10 else ''}\")  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ\n",
    "    \n",
    "    # ì°¨ì´ì  ë¶„ì„\n",
    "    print(f\"ğŸ’¡ í† í° ê°œìˆ˜: Standard({len(standard_tokens)}) vs N-gram({len(ngram_tokens)})\")\n",
    "    \n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nori í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜ ë°©ë²• ì•ˆë‚´\n",
    "print(\"\\nğŸ“‹ Nori í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜ ë°©ë²•\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "NoriëŠ” Elasticsearchì˜ ê³µì‹ í•œêµ­ì–´ ë¶„ì„ í”ŒëŸ¬ê·¸ì¸ì…ë‹ˆë‹¤.\n",
    "í˜„ì¬ Docker í™˜ê²½ì—ì„œ Norië¥¼ ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒ ë°©ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”:\n",
    "\n",
    "ğŸ› ï¸ ë°©ë²• 1: Docker Compose ìˆ˜ì •\n",
    "docker-compose.yaml íŒŒì¼ì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •:\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  elasticsearch:\n",
    "    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.2\n",
    "    container_name: elasticsearch-local\n",
    "    environment:\n",
    "      - discovery.type=single-node\n",
    "      - ES_JAVA_OPTS=-Xms1g -Xmx1g\n",
    "      - xpack.security.enabled=false\n",
    "      - xpack.security.http.ssl.enabled=false\n",
    "    ulimits:\n",
    "      memlock:\n",
    "        soft: -1\n",
    "        hard: -1\n",
    "    volumes:\n",
    "      - esdata:/usr/share/elasticsearch/data\n",
    "    ports:\n",
    "     - \"9200:9200\"\n",
    "     - \"9300:9300\"\n",
    "    command: >\n",
    "      /bin/bash -c \"\n",
    "      elasticsearch-plugin install analysis-nori &&\n",
    "      /usr/local/bin/docker-entrypoint.sh eswrapper\n",
    "      \"\n",
    "```\n",
    "\n",
    "ğŸ› ï¸ ë°©ë²• 2: ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆì— ì„¤ì¹˜\n",
    "```bash\n",
    "# ì»¨í…Œì´ë„ˆì— ì ‘ì†\n",
    "docker exec -it elasticsearch-local bash\n",
    "\n",
    "# Nori í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜\n",
    "elasticsearch-plugin install analysis-nori\n",
    "\n",
    "# Elasticsearch ì¬ì‹œì‘\n",
    "exit\n",
    "docker-compose restart elasticsearch\n",
    "```\n",
    "\n",
    "ğŸ› ï¸ ë°©ë²• 3: ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë¹Œë“œ\n",
    "Dockerfileì„ ë§Œë“¤ì–´ Noriê°€ í¬í•¨ëœ ì´ë¯¸ì§€ ìƒì„±\n",
    "\n",
    "âš ï¸ í˜„ì¬ ìƒí™©ì—ì„œëŠ” N-gram ë°©ì‹ìœ¼ë¡œë„ ì¶©ë¶„íˆ í•œê¸€ BM25 ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ’¡ N-gram vs Nori ë¹„êµ\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\"\"\n",
    "âœ… N-gram ë°©ì‹ (í˜„ì¬ êµ¬í˜„):\n",
    "- ì„¤ì¹˜ ë¶ˆí•„ìš”, ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥\n",
    "- ë¶€ë¶„ ë§¤ì¹­ì— ê°•í•¨ (\"ê¸‰ì‹\" ê²€ìƒ‰ìœ¼ë¡œ \"ë“ ë“ ê¸‰ì‹\" ì°¾ê¸°)\n",
    "- 2-3ê¸€ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ í™•ì¥\n",
    "- ë…¸ì´ì¦ˆê°€ ë§ì„ ìˆ˜ ìˆìŒ\n",
    "\n",
    "âœ… Nori ë°©ì‹ (ê¶Œì¥):\n",
    "- í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬\n",
    "- ë¶ˆìš©ì–´ ì œê±°ë¡œ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "- ë³µí•©ì–´ ë¶„í•´ (ì˜ˆ: \"ë“ ë“ ê¸‰ì‹\" â†’ \"ë“ ë“ \" + \"ê¸‰ì‹\")\n",
    "- ë” ì •êµí•œ í•œê¸€ ê²€ìƒ‰ ì§€ì›\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ¯ ê²°ë¡ : N-gramìœ¼ë¡œë„ ê¸°ë³¸ ëŒ€ë¹„ í° ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œê¸€ ìµœì í™” ì¸ë±ìŠ¤ì— ë¬¸ì„œ ì¸ë±ì‹±\n",
    "print(\"í•œê¸€ ìµœì í™” ì¸ë±ìŠ¤ì— ë¬¸ì„œ ì¸ë±ì‹± ì¤‘...\")\n",
    "for i, doc in enumerate(docs):\n",
    "    document = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"chunk_id\": i\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # í•œê¸€ ìµœì í™” ì¸ë±ìŠ¤ì— ë¬¸ì„œ ì¸ë±ì‹±\n",
    "        response = es.index(index=korean_index_name, id=i, body=document)\n",
    "        print(f\"í•œê¸€ ìµœì í™” Chunk {i} ì¸ë±ì‹± ì™„ë£Œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"Chunk {i} ì¸ë±ì‹± ì‹¤íŒ¨: {e}\")\n",
    "        break\n",
    "\n",
    "# í•œê¸€ ìµœì í™” ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨\n",
    "try:\n",
    "    es.indices.refresh(index=korean_index_name)\n",
    "    print(f\"âœ… í•œê¸€ ìµœì í™” ì¸ë±ìŠ¤ì— ì´ {len(docs)}ê°œì˜ chunkê°€ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œê¸€ ìµœì í™” BM25 ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def korean_bm25_search(query, top_k=3):\n",
    "    \"\"\"\n",
    "    í•œê¸€ ìµœì í™” BM25 ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    \n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰ ì¿¼ë¦¬\n",
    "        top_k (int): ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        list: ê²€ìƒ‰ ê²°ê³¼ì™€ ì ìˆ˜\n",
    "    \"\"\"\n",
    "    search_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": {\n",
    "                    \"query\": query,\n",
    "                    \"operator\": \"or\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"content\", \"chunk_id\"],\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"content\": {\n",
    "                    \"fragment_size\": 150,\n",
    "                    \"number_of_fragments\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # í•œê¸€ ìµœì í™” ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ ì‹¤í–‰\n",
    "        response = es.search(index=korean_index_name, body=search_body)\n",
    "        \n",
    "        results = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            result = {\n",
    "                'chunk_id': hit['_source']['chunk_id'],\n",
    "                'score': hit['_score'],\n",
    "                'content': hit['_source']['content'],\n",
    "                'highlight': hit.get('highlight', {}).get('content', [])\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return []\n",
    "\n",
    "# Standard vs Nori ë¶„ì„ê¸° ì„±ëŠ¥ ë¹„êµ\n",
    "print(\"ğŸ” Standard vs Nori ë¶„ì„ê¸° ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_queries = [\n",
    "    \"ì„œìš¸ë“ ë“ ê¸‰ì‹\",\n",
    "    \"ì‹ì¬ë£Œ ì£¼ë¬¸\", \n",
    "    \"ì–´ë¦°ì´ì§‘\",\n",
    "    \"ê¸‰ì‹ ê°œí¸ì‚¬í•­\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nğŸ“ ê²€ìƒ‰ì–´: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Standard ë¶„ì„ê¸° ê²°ê³¼\n",
    "    print(\"ğŸ”¹ Standard ë¶„ì„ê¸° (ê¸°ë³¸):\")\n",
    "    standard_results = bm25_search(query, top_k=3)\n",
    "    if standard_results:\n",
    "        for i, result in enumerate(standard_results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "    \n",
    "    # Nori ë¶„ì„ê¸° ê²°ê³¼ (í•œê¸€ ìµœì í™”)\n",
    "    print(\"ğŸ”¹ Nori ë¶„ì„ê¸° (í•œê¸€ ìµœì í™”):\")\n",
    "    korean_results = korean_bm25_search(query, top_k=3)\n",
    "    if korean_results:\n",
    "        for i, result in enumerate(korean_results, 1):\n",
    "            print(f\"  [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "    \n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë¶„ì„ ê²°ê³¼ ë¹„êµ\n",
    "print(\"\\nğŸ” í† í¬ë‚˜ì´ì € ë¶„ì„ ê²°ê³¼ ë¹„êµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë¶„ì„í•  í…ìŠ¤íŠ¸ ìƒ˜í”Œ\n",
    "sample_texts = [\n",
    "    \"ì„œìš¸ë“ ë“ ê¸‰ì‹\",\n",
    "    \"ì‹ì¬ë£Œ ì£¼ë¬¸ ë°©ë²•\",\n",
    "    \"ì–´ë¦°ì´ì§‘ ëŒ€ìƒ ì‹ ê·œ íšŒì›ê°€ì…\",\n",
    "    \"ë“ ë“ ê¸‰ì‹ ê°œí¸ì‚¬í•­ ì•ˆë‚´\"\n",
    "]\n",
    "\n",
    "def analyze_text(text, analyzer_name, index_name):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í† í° ê²°ê³¼ë¥¼ ë°˜í™˜\"\"\"\n",
    "    try:\n",
    "        response = es.indices.analyze(\n",
    "            index=index_name,\n",
    "            body={\n",
    "                \"analyzer\": analyzer_name,\n",
    "                \"text\": text\n",
    "            }\n",
    "        )\n",
    "        tokens = [token['token'] for token in response['tokens']]\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "for text in sample_texts:\n",
    "    print(f\"\\nğŸ“ ë¶„ì„ í…ìŠ¤íŠ¸: '{text}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Standard ë¶„ì„ê¸° ê²°ê³¼\n",
    "    standard_tokens = analyze_text(text, \"standard\", index_name)\n",
    "    print(f\"ğŸ”¹ Standard: {standard_tokens}\")\n",
    "    \n",
    "    # Nori ë¶„ì„ê¸° ê²°ê³¼\n",
    "    korean_tokens = analyze_text(text, \"nori_analyzer\", korean_index_name)\n",
    "    print(f\"ğŸ”¹ Nori:     {korean_tokens}\")\n",
    "    \n",
    "    # ì°¨ì´ì  ë¶„ì„\n",
    "    if len(korean_tokens) != len(standard_tokens):\n",
    "        print(f\"ğŸ’¡ í† í° ê°œìˆ˜ ì°¨ì´: Standard({len(standard_tokens)}) vs Nori({len(korean_tokens)})\")\n",
    "    \n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ ëŒ€ì•ˆ ë°©ë²• (Noriê°€ ì—†ëŠ” ê²½ìš°)\n",
    "ngram_index_name = \"bm25_ngram_documents\"\n",
    "\n",
    "# N-gram ê¸°ë°˜ ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜\n",
    "ngram_mapping = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"similarity\": {\n",
    "                \"ngram_bm25_similarity\": {\n",
    "                    \"type\": \"BM25\",\n",
    "                    \"k1\": 1.2,\n",
    "                    \"b\": 0.75\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"korean_ngram_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"keyword\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"korean_ngram_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"korean_ngram_filter\": {\n",
    "                    \"type\": \"ngram\",\n",
    "                    \"min_gram\": 2,\n",
    "                    \"max_gram\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"ngram_bm25_similarity\",\n",
    "                \"analyzer\": \"korean_ngram_analyzer\",\n",
    "                \"search_analyzer\": \"korean_ngram_analyzer\"\n",
    "            },\n",
    "            \"chunk_id\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# N-gram ì¸ë±ìŠ¤ ìƒì„±\n",
    "if es.indices.exists(index=ngram_index_name):\n",
    "    es.indices.delete(index=ngram_index_name)\n",
    "    print(f\"ê¸°ì¡´ N-gram ì¸ë±ìŠ¤ '{ngram_index_name}'ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "es.indices.create(index=ngram_index_name, body=ngram_mapping)\n",
    "print(f\"âœ… N-gram ì¸ë±ìŠ¤ '{ngram_index_name}'ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# N-gram ì¸ë±ìŠ¤ì— ë¬¸ì„œ ì¸ë±ì‹±\n",
    "for i, doc in enumerate(docs):\n",
    "    document = {\n",
    "        \"content\": doc.page_content,\n",
    "        \"chunk_id\": i\n",
    "    }\n",
    "    es.index(index=ngram_index_name, id=i, body=document)\n",
    "\n",
    "es.indices.refresh(index=ngram_index_name)\n",
    "print(f\"âœ… N-gram ì¸ë±ìŠ¤ì— {len(docs)}ê°œì˜ chunkê°€ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ê°€ì§€ í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ì¢…í•© ë¹„êµ\n",
    "def ngram_bm25_search(query, top_k=3):\n",
    "    \"\"\"N-gram BM25 ê²€ìƒ‰\"\"\"\n",
    "    search_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"content\": {\n",
    "                    \"query\": query,\n",
    "                    \"operator\": \"or\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"content\", \"chunk_id\"]\n",
    "    }\n",
    "    \n",
    "    response = es.search(index=ngram_index_name, body=search_body)\n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result = {\n",
    "            'chunk_id': hit['_source']['chunk_id'],\n",
    "            'score': hit['_score'],\n",
    "            'content': hit['_source']['content']\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "print(\"\\nğŸ† 3ê°€ì§€ í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ì¢…í•© ë¹„êµ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_queries = [\n",
    "    \"ì„œìš¸ë“ ë“ ê¸‰ì‹\",\n",
    "    \"ê¸‰ì‹ ê°œí¸\",\n",
    "    \"ì‹ì¬ë£Œ ì£¼ë¬¸\",\n",
    "    \"ì–´ë¦°ì´ì§‘ íšŒì›ê°€ì…\"\n",
    "]\n",
    "\n",
    "for query in comparison_queries:\n",
    "    print(f\"\\nğŸ” ê²€ìƒ‰ì–´: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Standard ë¶„ì„ê¸°\n",
    "    print(\"1ï¸âƒ£ Standard ë¶„ì„ê¸° (ê¸°ë³¸):\")\n",
    "    standard_results = bm25_search(query, top_k=2)\n",
    "    if standard_results:\n",
    "        for i, result in enumerate(standard_results, 1):\n",
    "            print(f\"   [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "            print(f\"       ë‚´ìš©: {result['content'][:80]}...\")\n",
    "    else:\n",
    "        print(\"   ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "    \n",
    "    # 2. Nori ë¶„ì„ê¸° (í•œê¸€ í˜•íƒœì†Œ ë¶„ì„)\n",
    "    print(\"\\\\n2ï¸âƒ£ Nori ë¶„ì„ê¸° (í•œê¸€ í˜•íƒœì†Œ ë¶„ì„):\")\n",
    "    korean_results = korean_bm25_search(query, top_k=2)\n",
    "    if korean_results:\n",
    "        for i, result in enumerate(korean_results, 1):\n",
    "            print(f\"   [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "            print(f\"       ë‚´ìš©: {result['content'][:80]}...\")\n",
    "    else:\n",
    "        print(\"   ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (Nori í”ŒëŸ¬ê·¸ì¸ ë¯¸ì„¤ì¹˜)\")\n",
    "    \n",
    "    # 3. N-gram ë¶„ì„ê¸°\n",
    "    print(\"\\\\n3ï¸âƒ£ N-gram ë¶„ì„ê¸° (2-3ê¸€ì ë‹¨ìœ„):\")\n",
    "    ngram_results = ngram_bm25_search(query, top_k=2)\n",
    "    if ngram_results:\n",
    "        for i, result in enumerate(ngram_results, 1):\n",
    "            print(f\"   [{i}] Chunk {result['chunk_id']} - ì ìˆ˜: {result['score']:.4f}\")\n",
    "            print(f\"       ë‚´ìš©: {result['content'][:80]}...\")\n",
    "    else:\n",
    "        print(\"   ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elastic-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
